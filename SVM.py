# -*- coding: utf-8 -*-
"""425hw6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UcXMsOCCD9ooDaKwE3Uyz506HJ-hJnQS
"""

import numpy as np
from keras.datasets import mnist
from cvxopt import matrix as cvxopt_matrix, solvers as cvxopt_solvers

# Extract digits 0 and 9 from the dataset
def extract_digits(x, y, d_1, d_2):
    x_t = {'d_1': [], 'd_2': []}
    y_t = {'d_1': [], 'd_2': []}
    for i, d in enumerate(y):
        if d == d_1:
            x_t['d_1'].append(x[i].reshape(-1, 1))
            y_t['d_1'].append(-1)
        elif d == d_2:
            x_t['d_2'].append(x[i].reshape(-1, 1))
            y_t['d_2'].append(1)
    return x_t, y_t

def prepare_data(data_dict, label_dict):
    num_samples = len(data_dict['d_1']) + len(data_dict['d_2'])
    data_matrix = np.ones((num_samples, 785))
    labels = np.array(label_dict['d_1'] + label_dict['d_2'])[:, np.newaxis] * 1.

    for i in range(num_samples):
        digit = 'd_1' if i < len(data_dict['d_1']) else 'd_2'
        data_matrix[i, 1:] = data_dict[digit][i - len(data_dict['d_1'])].T

    return data_matrix, labels

def svm_dual(x, y):
    # Construct the Kernel matrix
    H = y*x @ (y*x).T
    m = x.shape[0]

    # Convert data into cvxopt format
    P = cvxopt_matrix(H)
    q = cvxopt_matrix(-np.ones((m, 1)))
    G = cvxopt_matrix(-np.eye(m))
    h = cvxopt_matrix(np.zeros(m))
    A = cvxopt_matrix(y.T)
    b = cvxopt_matrix(np.zeros(1))

    # Run solver
    sol = cvxopt_solvers.qp(P, q, G, h, A, b)
    alphas = np.array(sol['x'])

    # Calculate the weight vector w
    w = ((alphas).T @ (y*x)).T

    # Select support vectors
    S = (alphas > 1e-4).flatten()

    # Calculate the bias term
    b = y[S] - np.dot(x[S], w)
    return w, b

# Compute classification accuracy
def cl_accuracy(y, y_hat):
    return (np.sum(y_hat == y) / y.shape[0]) * 100

def main_real():

    # Load MNIST dataset
    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()
    d_1, d_2 = 0, 9

    # Extract digits 0 and 9 from training and testing data
    X_train_0_9, Y_train_0_9 = extract_digits(X_train, Y_train, d_1, d_2)
    X_test_0_9, Y_test_0_9 = extract_digits(X_test, Y_test, d_1, d_2)

    # Prepare data for training and testing
    X_Train_0_9, Y_Train_0_9 = prepare_data(X_train_0_9, Y_train_0_9)
    X_Test_0_9, Y_Test_0_9 = prepare_data(X_test_0_9, Y_test_0_9)

    # # Keep only useful features 
    # train_feature_var = np.var(X_Train_0_9, axis = 0)
    # X_Train_0_9 = X_Train_0_9[:, train_feature_var > 0]
    # X_Test_0_9  = X_Test_0_9[:, train_feature_var > 0]

    # Shuffle training data
    # shuffle_index = np.random.permutation(len(Y_Train_0_9))
    # X_Train_0_9, Y_Train_0_9 = X_Train_0_9[shuffle_index], Y_Train_0_9[shuffle_index]

    w, b = svm_dual(X_Train_0_9, Y_Train_0_9)

    # Compute predictions
    y_test_hat = np.sign(X_Test_0_9 @ w) # add bias if not empty
    y_train_hat = np.sign(X_Train_0_9 @ w) # add bias if not empty

    # Calculate accuracies
    train_accuracy = cl_accuracy(Y_Train_0_9, y_train_hat)
    test_accuracy = cl_accuracy(Y_Test_0_9, y_test_hat)

    # Display results
    print('Train Accuracy: {} Test Accuracy: {}'.format(train_accuracy, test_accuracy))

# Generate synthetic data
def synthetic_data(mean_0, mean_1, cov, m, l):
    y = np.random.binomial(1, 0.5, (m, 1))*1.0
    x = np.ones((m, l))
    x[y[:, 0] == 0, 1:] = np.random.multivariate_normal(mean_0, cov, np.sum(y == 0))
    x[y[:, 0] == 1, 1:] = np.random.multivariate_normal(mean_1, cov, np.sum(y == 1))
    y[y[:,0] == 0, 0] = -1.0 # extra step only for SVM
    return x, y

# Iterate over different training data sizes
def main_synthetic():

    # Parameters
    mean_0 = [-3, -3]
    mean_1 = [2, 2]
    cov = [[1, 0], [0, 10]]
    l = 3
    m_train = 100
    m_test = 100

    # Generate train and test data
    x_train, y_train = synthetic_data(mean_0, mean_1, cov, m_train, l)
    x_test, y_test = synthetic_data(mean_0, mean_1, cov, m_test, l)

    # Data visualization
    plt.figure()
    plt.scatter(x_train[:,1], x_train[:,2], c=y_train, marker='x')
    plt.show()
    plt.figure()
    plt.scatter(x_test[:,1], x_test[:,2], c=y_test, marker='x')
    plt.show()
     
    w, b = svm_dual(x_train, y_train)
    print(w.shape)

    # Compute predictions
    y_test_hat  = np.sign(x_test @ w + b[0]) # add bias if not empty
    y_train_hat = np.sign(x_train @ w + b[0]) # add bias if not empty

    # Calculate accuracies
    train_accuracy = cl_accuracy(y_train, y_train_hat)
    test_accuracy  = cl_accuracy(y_test, y_test_hat)

    # Display results
    print('Train Accuracy: {} Test Accuracy: {}'.format(train_accuracy, test_accuracy))

synthetic = 0

if __name__ == "__main__":
   if synthetic == 1:
      main_synthetic()
   else:
      main_real()
